[training]
batch_size = 64
gradient_accumulation_steps = 8
total_round = 5
drop_out = 0.5
num_workers = 1
step1_epochs = 10
step2_epochs = 10
num_protos = 1
device0 = cuda:3
device1 = cuda:4    
device2 = cuda:5
device3 = cuda:6
seed = 100
max_grad_norm = 10
task_length = 8
kl_temp = 2
temp = 0.1
infonce_lossfactor = 0.5

[Encoder]
bert_path = ./bert-base-uncased
max_length = 256
vocab_size = 32000
hidden_size = 4096
marker_size = 4
pattern = entity_marker
encoder_output_size = 4096
output_size = 4096


[dropout]
drop_p = 0.1
f_pass = 10
kappa_neg = 0.03
kappa_pos = 0.05


[scheduler]
T_mult = 1
rewarm_epoch_num = 2
# StepLR
decay_rate = 0.9
decay_steps = 800
